# -*- coding: utf-8 -*-
"""finetuneinceptionV3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AxXYHEXTRS_62-FSJdA53PImcFy92voC
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import os
import pickle
import itertools
import io
import time
import bson
import threading
import random
import pandas as pd
# from scipy.misc import imread
import numpy as np
from sklearn.preprocessing import LabelEncoder
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras import backend as K
import keras
from keras.applications.inception_v3 import preprocess_input

# def create_model():
#     # create the base pre-trained model
#     base_model = InceptionV3(weights='imagenet', include_top=False,input_shape = (500,500,3))
    
#     # add a global spatial average pooling layer
#     x = base_model.output
#     x = Dropout(0.5)(x)
#     x = GlobalAveragePooling2D()(x)
#     # and a logistic layer -- let's say we have 200 classes
#     predictions = Dense(6, activation='softmax')(x)

#     # this is the model we will train
#     model = Model(inputs=base_model.input, outputs=predictions)

#     # first: train only the top layers (which were randomly initialized)
#     # i.e. freeze all convolutional InceptionV3 layers
#     # for layer in base_model.layers[:249]:
#     #     layer.trainable = False
#     # for layer in base_model.layers[249:]:
#     #     layer.trainable = True
#     for layer in base_model.layers:
#         layer.trainable = True

#     # compile the model (should be done *after* setting layers to non-trainable)
#     model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
#     return model

# from google.colab import drive
# drive.mount('/content/drive')

# with open('/content/drive/My Drive/wiki_pages.csv', 'r') as f:
#   df = pd.read_csv(f)

df = pd.read_csv('wiki_pages.csv')
# with open('/content/drive/My Drive/wikipages_SplToken1.csv', 'r') as f:
#   df = pd.read_csv(f)

# l2 = os.listdir('/content/drive/My Drive/file2/')
l2 = os.listdir('file1/')

l1 = []
for i in range(len(df)):
    l1.append(df['Name'][i])

c=0
l3 = []
l4 = []
l5 = []
for i in l1:
  j = i+'.jpg'
  if j in l2:
    l3.append(c)
    l4.append(df['Label'][c])
    l5.append(df['Name'][c])
  c=c+1

def grouper(n, iterable):
    '''
    Given an iterable, it'll return size n chunks per iteration.
    Handles the last chunk too.
    '''
    it = iter(iterable)
    while True:
        chunk = tuple(itertools.islice(it, n))
        if not chunk:
            return
        yield chunk

# class threadsafe_iter:
#     """
#     Takes an iterator/generator and makes it thread-safe by
#     serializing call to the `next` method of given iterator/generator.
#     """
#     def __init__(self, it):
#         self.it = it
#         self.lock = threading.Lock()

#     def __iter__(self):
#         return self

#     def __next__(self):
#         with self.lock:
#             return self.it.__next__()

# def threadsafe_generator(f):
#     """
#     A decorator that takes a generator function and makes it thread-safe.
#     """
#     def g(*a, **kw):
#         return threadsafe_iter(f(*a, **kw))
#     return g

# @threadsafe_generator
# def get_features_label(documents, batch_size=32, return_labels=True):
#     '''
#     Given a document return X, y
    
#     X is scaled to [0, 1] and consists of all images contained in document.
#     y is given an integer encoding.
#     '''
    
    
#     for batch in grouper(batch_size, documents): 
#         images = []
#         labels = []

#         for document in batch:
#             label = document[1]
#             img_path = document[0]
#             try:
#               img = image.load_img(img_path,target_size =(2000,1000))
#             except:
#               print('loading error')
#     # continue
#             x = image.img_to_array(img)
#             x = np.expand_dims(x, axis = 0)
#             x = preprocess_input(x)
#             x = x[0]
#             images.append(x)
#             labels.append(label)

       
#         yield np.array(images), np.array(labels)

def get_features_label(documents, batch_size=16, return_labels=True):
    '''
    Given a document return X, y
    
    X is scaled to [0, 1] and consists of all images contained in document.
    y is given an integer encoding.
    '''
    start = 0
    labels = []
    images = []
    while True:
        del images, labels
        
        if(start + batch_size > len(documents)):
            # random.shuffle(documents)
            start = 0
    # for batch in grouper(batch_size, documents): 
        images = []
        labels = []

        for document in documents[start: start + batch_size]:
            label = document[1]
            img_path = document[0]
            try:
              img = image.load_img(img_path,target_size =(500,500))
            except:
              print('loading error')
    # continue
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis = 0)
            x = preprocess_input(x)
            x = x[0]
            images.append(x)
            labels.append(label)

        start += batch_size
        yield np.array(images), np.array(labels)

import keras

class My_Custom_Generator(keras.utils.Sequence) :
  
    def __init__(self, documents, batch_size=16) :
        self.documents = documents
        self.batch_size = batch_size


    def __len__(self) :
        return (np.ceil(len(self.documents) / float(self.batch_size))).astype(np.int)


    def __getitem__(self, idx) :
        labels = []
        images = []
        for document in self.documents[idx * self.batch_size : (idx+1) * self.batch_size]:
            label = document[1]
            img_path = document[0]
            try:
              img = image.load_img(img_path,target_size =(2000,1000))
            except:
              print('loading error')
    # continue
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis = 0)
            x = preprocess_input(x)
            x = x[0]
            images.append(x)
            labels.append(label)

        # start += batch_size
        return np.array(images), np.array(labels)

labels = []
c1 = 0
c2 = 0
c3 = 0
c4 = 0 
c5 = 0
c6 = 0
for i in range(len(l3)):
  a = l4[i]
  if(a == 'FA'):
    labels.append(0)
    c1+=1
  if(a == 'GA'):
    labels.append(1)
    c2+=1
  if(a == 'B'):
    labels.append(2)
    c3+=1
  if(a == 'C'):
    labels.append(3)
    c4+=1
  if(a == 'Start'):
    labels.append(4)
    c5+=1
  if(a == 'Stub'):
    labels.append(5)
    c6+=1

proc = []
for i in range(len(l5)):
#   j = '/content/drive/My Drive/file2/' + l5[i] + '.jpg'
  j = 'file1/' + l5[i] + '.jpg'
  k = labels[i]
  proc.append((j,k))

len(proc)



len1 = 30000
train = proc[:20000]
val = proc[20000:24000]
test = proc[24000:30000]
# len1 = int(0.8*len(proc))
# train = proc[:len1]
# val = proc[len1:]

# inception = create_model()

# callback = keras.callbacks.TensorBoard(
#     log_dir='./logs/inception/2/{}'.format(time.time())
# )
train_generator = get_features_label(train, batch_size=16)
val_generator = get_features_label(val, batch_size=16)
test_generator = get_features_label(test, batch_size=16)
# train_generator = My_Custom_Generator(train)
# val_generator = My_Custom_Generator(val)

# a = [1,2,3,4]
# a[-1:]

# for layer in inception.layers[:249]:
#     layer.trainable = False
# for layer in inception.layers[249:]:
#     layer.trainable = True

from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)

# inception.compile(optimizer=Adam(lr=0.0001),
#                             loss='sparse_categorical_crossentropy',
#                             metrics=['accuracy'])

# # for layer in inception.layers[:-1]:
#     layer.trainable = False
# for layer in inception.layers[-1:]:
#     layer.trainable = True

# we need to recompile the model for these modifications to take effect
# we use SGD with a low learning rate


# we train our model again (this time fine-tuning the top 2 inception blocks
# alongside the top Dense layers
# So we can look at the progress on Tensorboard
# callback = keras.callbacks.TensorBoard(
#     log_dir='./logs/inception/{}'.format(time.time())
# )

# generator = get_features_label(bson.decode_file_iter(open('data/train.bson', 'rb')))

# docs says train for a few epocs (LOL!)
# Each step is 32 images.

# 200 epochs x  steps x 32 images -> 320 000 images / ~7M
# train_steps = int(len(train)/16)
# val_steps = int(len(val)/16)
# inception.fit_generator(
#     generator=train_generator,
#     epochs=2,
#     steps_per_epoch=train_steps,
#     callbacks=[es],
#     validation_data=val_generator,
#     validation_steps=val_steps
# )

test_steps = int(len(test)/16)

print(test_steps)

from keras.models import load_model

# inception.save('/content/drive/My Drive/ft.h5')
# inception.save('inception_ftune_2.h5')
# inception = load_model('/content/drive/My Drive/inception_ftune.h5')
inception = load_model('inception_ftune_2.h5')

predIdxs = inception.predict_generator(test_generator,
	steps=test_steps)
predIdxs = np.argmax(predIdxs, axis=1)

inception.evaluate_generator(test_generator,steps=test_steps)

print(len(predIdxs))

l1 = []
for i in test:
  l1.append(i[1])

len(l1)

c =0
for i in range(len(l1)):
  if(l1[i] == predIdxs[i]):
    c+=1

inception.evaluate_generator(test_generator,steps=test_steps)
print(c)
print('accuracy is: ',c/len(l1))

# from keras import backend as K

# # with a Sequential model
# layer2 = K.function([inception.layers[0].input],
#                                   [inception.layers[-2].output])

# l9=[]
# c=0
# for i in val:
#     img_path = i[0]
#     try:
#         img = image.load_img(img_path,target_size =(500,500))
#     except:
#         print('loading error')
#         # continue
#     x = image.img_to_array(img)
#     x = np.expand_dims(x, axis = 0)
#     x = preprocess_input(x)
#     # print(x.shape)
#     # x = x[0]

#     layer_output = layer2([x])[0][0]
#     l9.append(layer_output)
#     c+=1
#     print(c)

# len(l9[0])

# inception.summary()

# for i in

# val_generator

# len(val_generator)

# for i, batch in enumerate(val_generator):
#   print(i,len(batch))

# val_generator = get_features_label(val, batch_size=16,return_labels=False)

# predictions = []

# for i, batch in enumerate(val_generator):
#     output = inception.predict(batch)
#     labels = labelencoder.inverse_transform(output.argmax(axis=1))
#     predictions.extend(labels.tolist())

# inception.summary()

# layer

# inception.layers[-4]

# len(inception.layers)

